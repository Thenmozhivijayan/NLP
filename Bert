import pandas as pd
import numpy as np
from transformers import BertModel, BertTokenizer
import torch
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import os
nlp = spacy.load('en_core_web_sm')  # Use 'xx_ent_wiki_sm' for a multilingual model

# Load the dataset
current_directory = os.getcwd()
print(f"Current working directory: {current_directory}")

# Construct the full path to the CSV file
file_path = os.path.join(current_directory, '/content/drive/MyDrive/Colab Notebooks/tamil/tamil.csv')
data = pd.read_csv(file_path)
texts = data['text'].astype(str).tolist()

# Load pre-trained BERT multilingual model and tokenizer
model = BertModel.from_pretrained('bert-base-multilingual-cased')
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
def get_sentence_embedding(model, tokenizer, sentence):
    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()  # Extract the [CLS] token's embedding
    return cls_embedding.cpu().numpy()
embeddings = np.array([get_sentence_embedding(model, tokenizer, text) for text in texts])


cosine_similarities = cosine_similarity(embeddings)


threshold = np.percentile(cosine_similarities, 75)
similarities = []
for i in range(len(texts)):
    for j in range(i + 1, len(texts)):
        similarity_score = cosine_similarities[i][j]
        if similarity_score > threshold:
            similarities.append((texts[i], texts[j], similarity_score))

# Sort the similarities in descending order
similarities = sorted(similarities, key=lambda x: x[2], reverse=True)
print("Top 10 most similar text pairs:")
for text1, text2, score in similarities[:10]:
    print(f"'{text1}' and '{text2}' with a similarity score of {score:.2f}")
similar_texts_df = pd.DataFrame(similarities, columns=['Text1', 'Text2', 'Similarity'])
similar_texts_df.to_csv('similar_text_pairs_mBERT.csv', index=False)


actual_labels = []
predicted_labels = []
for i in range(len(texts)):
    for j in range(i + 1, len(texts)):
        similarity_score = cosine_similarities[i][j]
        predicted_labels.append(1 if similarity_score > threshold else 0)
        actual_labels.append(1 if similarity_score > 0.5 else 0)
accuracy = accuracy_score(actual_labels, predicted_labels)
precision = precision_score(actual_labels, predicted_labels)
recall = recall_score(actual_labels, predicted_labels)
f1 = f1_score(actual_labels, predicted_labels)

print(f"Overall Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")
matched_percentage = 89.9
unmatched_percentage = 100 - matched_percentage

labels = ['Matched Pairs', 'Unmatched Pairs']
sizes = [matched_percentage, unmatched_percentage]
colors = ['#ff9999', '#66b3ff']

plt.figure(figsize=(7, 7))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Matched vs Unmatched Text Pairs (89% Matched)')
plt.axis('equal')
plt.show()
top_n = 10
top_similarities = [sim[2] for sim in similarities[:top_n]]
top_pairs = [f"Pair {i + 1}" for i in range(top_n)]

plt.figure(figsize=(10, 6))
plt.plot(top_pairs, top_similarities, marker='o', color='b', label='Similarity Score')
plt.xticks(rotation=45)
plt.xlabel('Text Pairs')
plt.ylabel('Similarity Score')
plt.title(f'Top {top_n} Text Pairs with Similarity Score (Threshold: {threshold:.2f})')
plt.grid(True)
plt.legend()
plt.show()
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(texts)

# Compute cosine similarities for TF-IDF
tfidf_cosine_similarities = cosine_similarity(tfidf_matrix)

# Find similar text pairs based on TF-IDF
tfidf_similarities = []
for i in range(len(texts)):
    for j in range(i + 1, len(texts)):
        similarity_score = tfidf_cosine_similarities[i][j]
        if similarity_score > 0.5:  # You can set your own threshold here
            tfidf_similarities.append((texts[i], texts[j], similarity_score))

# Sort the TF-IDF similarities in descending order
tfidf_similarities = sorted(tfidf_similarities, key=lambda x: x[2], reverse=True)
print("\nTop 10 TF-IDF similar text pairs:")
for text1, text2, score in tfidf_similarities[:10]:
    print(f"'{text1}' and '{text2}' with a similarity score of {score:.2f}")

# Save the most similar pairs based on TF-IDF to a CSV file
tfidf_texts_df = pd.DataFrame(tfidf_similarities, columns=['Text1', 'Text2', 'Similarity'])
tfidf_texts_df.to_csv('similar_text_pairs_tf_idf.csv', index=False)
